---
title: "Modeling"
author: "Matthew Ross"
date: "2024-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(dplyr)
library(tidyr)
library(mgcv)
library(tidygeocoder)
```

```{r}
d <- read.csv("intermediate_for_modeling.csv")

unique_towns <- d %>%
  distinct(Town) %>%
  filter(!is.na(Town)) %>%
  mutate(full_address = paste0(Town, ", California"))

# Geocode towns to get latitude and longitude
town_coords <- unique_towns %>%
  geocode(address = full_address, method = "osm") %>%
  rename(lat.census.town = lat, lon.census.town = long)

# Merge geocoded coordinates back into the main dataframe
d <- d %>%
  left_join(town_coords, by = "Town")

d_new <- d
predictor_data <- read.csv("predictor_data.csv")
```

```{r}
# Load required libraries
library(dplyr)
library(tidyr)
library(lubridate)
library(zoo)
library(geosphere)

# Step 1: Ensure numeric columns in d_new are valid
d_new <- d_new %>%
  mutate(across(starts_with("Estimate."), ~ as.numeric(.), .names = "{.col}"))  # Convert non-numeric to NA

# Step 2: Ensure Year_Month is in Date format and create 6-month periods in d_new
d_new <- d_new %>%
  mutate(
    Year_Month = as.Date(paste0(Year_Month, "-01")),
    Period = floor_date(Year_Month, unit = "6 months")
  )

# Step 3: Ensure Year_Month and Period in predictor_data and validate numeric columns
predictor_data <- predictor_data %>%
  mutate(
    Year_Month = as.Date(paste0(Year_Month, "-01")),
    Period = floor_date(Year_Month, unit = "6 months")
  ) %>%
  mutate(across(
    starts_with("Estimate.") | matches("Opportunity_Zone"),
    ~ as.numeric(.),
    .names = "{.col}"
  ))  # Convert non-numeric to NA

d_aggregated <- d_new %>%
  mutate(
    Asset_Class = case_when(
      Primary.Asset.Type %in% c("Residential", "Industrial", "Office", "Retail") ~ Primary.Asset.Type,
      Primary.Asset.Type %in% c("Niche", "Mixed Use", "Land", "Hotel") ~ "Other",
      TRUE ~ "Unknown"
    )
  ) %>%
  group_by(Town, Period) %>%
  summarise(
    Deal_Volume = n(),
    Avg_Deal_Size = ifelse(Deal_Volume > 0, mean(Deal.Size.Usd.Mn., na.rm = TRUE), NA),
    Total_Size_Sq_Ft = ifelse(Deal_Volume > 0, mean(Total.Size.Sq.Ft., na.rm = TRUE), NA),
    Fed_Rate = mean(Fed_Rate, na.rm = TRUE),
    Unemployment_Rate = mean(Unemployment_Rate, na.rm = TRUE),
    HomeValue = mean(HomeValue, na.rm = TRUE),
    across(starts_with("Estimate."), ~ mean(., na.rm = TRUE), .names = "{.col}"),
    Opportunity_Zone = first(Opportunity_Zone),
    lat.census.town = first(lat.census.town),
    lon.census.town = first(lon.census.town),
    Residential_Percentage = sum(Asset_Class == "Residential") / Deal_Volume * 100,
    Industrial_Percentage = sum(Asset_Class == "Industrial") / Deal_Volume * 100,
    Office_Percentage = sum(Asset_Class == "Office") / Deal_Volume * 100,
    Retail_Percentage = sum(Asset_Class == "Retail") / Deal_Volume * 100,
    Other_Percentage = sum(Asset_Class == "Other") / Deal_Volume * 100,
    .groups = "drop"
  )

# Step 5: Fill in missing periods for each Town
full_grid <- expand.grid(
  Town = unique(d_new$Town),
  Period = seq(min(d_new$Period), max(d_new$Period), by = "6 months")
)

d_aggregated <- full_grid %>%
  left_join(d_aggregated, by = c("Town", "Period"))

# Step 6: Set percentages and metrics to 0 for periods with no deals
d_aggregated <- d_aggregated %>%
  mutate(
    Deal_Volume = ifelse(is.na(Deal_Volume), 0, Deal_Volume),
    Avg_Deal_Size = ifelse(Deal_Volume == 0, 0, Avg_Deal_Size),
    Total_Size_Sq_Ft = ifelse(Deal_Volume == 0, 0, Total_Size_Sq_Ft),
    Residential_Percentage = ifelse(Deal_Volume == 0, 0, Residential_Percentage),
    Industrial_Percentage = ifelse(Deal_Volume == 0, 0, Industrial_Percentage),
    Office_Percentage = ifelse(Deal_Volume == 0, 0, Office_Percentage),
    Retail_Percentage = ifelse(Deal_Volume == 0, 0, Retail_Percentage),
    Other_Percentage = ifelse(Deal_Volume == 0, 0, Other_Percentage)
  )
# Step 7: Perform row-wise updates
d_aggregated <- d_aggregated %>%
  rowwise() %>%
  mutate(
    Fed_Rate = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$Fed_Rate[
        predictor_data$Period == Period
      ], na.rm = TRUE),
      Fed_Rate
    ),
    Unemployment_Rate = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$Unemployment_Rate[
        predictor_data$Period == Period
      ], na.rm = TRUE),
      Unemployment_Rate
    ),
    HomeValue = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$HomeValue[
        predictor_data$Period == Period
      ], na.rm = TRUE),
      HomeValue
    ),
    Opportunity_Zone = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$Opportunity_Zone[
        predictor_data$Town == Town & predictor_data$Period == Period
      ], na.rm = TRUE),
      Opportunity_Zone
    ),
    across(
      starts_with("Estimate."),
      ~ ifelse(
        Deal_Volume == 0,
        mean(predictor_data[[cur_column()]][
          predictor_data$Town == Town & predictor_data$Period == Period
        ], na.rm = TRUE),
        .
      )
    ),
    lat.census.town = first(d_new$lat.census.town[d_new$Town == Town]),
    lon.census.town = first(d_new$lon.census.town[d_new$Town == Town])
  ) %>%
  ungroup()

# Step 8: Forward-backward fill for HomeValue and Estimate.* columns
d_aggregated <- d_aggregated %>%
  group_by(Town) %>%
  mutate(
    HomeValue = ifelse(
      is.na(HomeValue),
      zoo::na.locf(HomeValue, na.rm = FALSE, fromLast = FALSE),
      HomeValue
    ),
    HomeValue = ifelse(
      is.na(HomeValue),
      zoo::na.locf(HomeValue, na.rm = FALSE, fromLast = TRUE),
      HomeValue
    ),
    across(
      starts_with("Estimate."),
      ~ ifelse(
        is.na(.),
        zoo::na.locf(.x, na.rm = FALSE, fromLast = FALSE),
        .
      )
    ),
    across(
      starts_with("Estimate."),
      ~ ifelse(
        is.na(.),
        zoo::na.locf(.x, na.rm = FALSE, fromLast = TRUE),
        .
      )
    )
  ) %>%
  ungroup()

# Step 9: Fill missing values from nearest town if a town has no data for HomeValue or Estimate.*
fill_from_nearest <- function(row, target_column) {
  if (!is.na(row[[target_column]])) {
    return(row[[target_column]])
  }
  
  current_town <- row$Town
  current_period <- row$Period
  current_lat <- row$lat.census.town
  current_lon <- row$lon.census.town
  
  valid_towns <- d_aggregated %>%
    filter(!is.na(!!sym(target_column)) & Period == current_period) %>%
    mutate(distance = distHaversine(cbind(lon.census.town, lat.census.town), 
                                    c(current_lon, current_lat))) %>%
    arrange(distance)
  
  if (nrow(valid_towns) > 0) {
    return(valid_towns[[target_column]][1])
  }
  
  return(NA)
}

# Apply nearest town logic
d_aggregated <- d_aggregated %>%
  rowwise() %>%
  mutate(
    HomeValue = ifelse(is.na(HomeValue), fill_from_nearest(cur_data(), "HomeValue"), HomeValue),
    across(
      starts_with("Estimate."),
      ~ ifelse(is.na(.), fill_from_nearest(cur_data(), cur_column()), .)
    )
  ) %>%
  ungroup()
```
```{r}
library(zoo)
library(geosphere)

# Forward-backward search for Avg_Deal_Size and Total_Size_Sq_Ft within each town
d_aggregated <- d_aggregated %>%
  group_by(Town) %>%
  mutate(
    Avg_Deal_Size = ifelse(
      is.na(Avg_Deal_Size),
      zoo::na.locf(Avg_Deal_Size, na.rm = FALSE, fromLast = FALSE),
      Avg_Deal_Size
    ),
    Avg_Deal_Size = ifelse(
      is.na(Avg_Deal_Size),
      zoo::na.locf(Avg_Deal_Size, na.rm = FALSE, fromLast = TRUE),
      Avg_Deal_Size
    ),
    Total_Size_Sq_Ft = ifelse(
      is.na(Total_Size_Sq_Ft),
      zoo::na.locf(Total_Size_Sq_Ft, na.rm = FALSE, fromLast = FALSE),
      Total_Size_Sq_Ft
    ),
    Total_Size_Sq_Ft = ifelse(
      is.na(Total_Size_Sq_Ft),
      zoo::na.locf(Total_Size_Sq_Ft, na.rm = FALSE, fromLast = TRUE),
      Total_Size_Sq_Ft
    )
  ) %>%
  ungroup()

# Define the function for nearest-town analysis
fill_from_nearest <- function(row, target_column) {
  if (!is.na(row[[target_column]])) {
    return(row[[target_column]])  # Return existing value if it's not NA
  }
  
  # Extract relevant information for the current row
  current_town <- row$Town
  current_period <- row$Period
  current_lat <- row$lat.census.town
  current_lon <- row$lon.census.town
  
  # Find the nearest town with valid data for the target column
  valid_towns <- d_aggregated %>%
    filter(!is.na(!!sym(target_column)) & Deal_Volume > 1 & Period == current_period) %>%
    mutate(distance = distHaversine(cbind(lon.census.town, lat.census.town), 
                                    c(current_lon, current_lat))) %>%
    arrange(distance)  # Sort by distance
  
  if (nrow(valid_towns) > 0) {
    return(valid_towns[[target_column]][1])  # Return the value from the nearest valid town
  }
  
  return(NA)  # Return NA if no valid data is found
}

# Apply nearest-town logic to Avg_Deal_Size and Total_Size_Sq_Ft
d_aggregated <- d_aggregated %>%
  rowwise() %>%
  mutate(
    Avg_Deal_Size = ifelse(is.na(Avg_Deal_Size), fill_from_nearest(cur_data(), "Avg_Deal_Size"), Avg_Deal_Size),
    Total_Size_Sq_Ft = ifelse(is.na(Total_Size_Sq_Ft), fill_from_nearest(cur_data(), "Total_Size_Sq_Ft"), Total_Size_Sq_Ft)
  ) %>%
  ungroup()

# saveRDS(d_aggregated, "data_for_modeling.rds")
```


```{r}
# opt <- options()
# options(pkgType="both")
# install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
# options(opt)
```


```{r}
# Load Required Libraries
library(dplyr)
library(glmnet)
library(INLA)
library(ggplot2)
library(tidyr)
library(car)    # For VIF
library(sf)     # For spatial data handling
library(ggmap)  # For enhanced spatial plots
library(tmaptools)  # For geocode_OSM

# ------------------------------
# Step 1: Load and Initialize Data
# ------------------------------
d_aggregated <- readRDS("data_for_modeling_with_coords.rds")
d_aggregated
# Create a new dataframe for modeling to preserve the original data
d_modeling <- d_aggregated

# ------------------------------
# Step 2: Data Preparation
# ------------------------------

# Define all numeric columns based on dataset structure (columns 5 to 57)
all_numeric_columns <- names(d_modeling)[4:57]

# Exclude latitude and longitude from numeric columns
numeric_columns <- setdiff(all_numeric_columns, c("lat.census.town", "lon.census.town"))

# Convert categorical variables to factors
d_modeling <- d_modeling %>%
  mutate(
    Town = as.factor(Town),
    Period = as.factor(Period)  # Convert to factor if categorical
  )

# Remove rows with NA in numeric predictors
na_rows <- d_modeling %>%
  select(all_of(numeric_columns)) %>%
  apply(1, function(x) any(is.na(x)))

d_modeling <- d_modeling[!na_rows, ]

# Create a numeric time variable if Period is categorical
d_modeling <- d_modeling %>%
  arrange(Period) %>%
  mutate(Period_numeric = as.numeric(as.factor(Period)))

# Scale numeric predictors
d_modeling <- d_modeling %>%
  mutate(across(all_of(numeric_columns), scale, .names = "{.col}_scaled"))

# Define the response variable
y <- d_modeling$Deal_Volume

# ------------------------------
# Step 3: LASSO Regression for Variable Selection
# ------------------------------

X <- model.matrix(
  ~ . -1,
  data = d_modeling %>%
    select(ends_with("_scaled"), Opportunity_Zone)
)

lasso_model <- cv.glmnet(X, y, alpha = 1, family = "poisson")
lasso_coefficients <- coef(lasso_model, s = "lambda.1se")
coefficients_matrix <- as.matrix(lasso_coefficients)
non_zero_indices <- which(coefficients_matrix != 0)
selected_predictors <- rownames(coefficients_matrix)[non_zero_indices]
selected_predictors <- selected_predictors[selected_predictors != "(Intercept)"]

# ------------------------------
# Step 4: Consolidation of Predictors
# ------------------------------

d_modeling <- d_modeling %>%
  mutate(
    Car_scaled = Estimate.Car..truck..or.van....carpooled_scaled,
    Other_Transportation_scaled = Estimate.Public.transportation..excluding.taxicab._scaled + Estimate.Walked_scaled,
    Children_scaled = Estimate.Own.children.under.6.years_scaled + Estimate.Own.children.6.to.17.years_scaled,
    Corporate_scaled = Estimate.Sales.and.office.occupations_scaled +
      Estimate.Information_scaled +
      Estimate.Finance.and.insurance..and.real.estate.and.rental.and.leasing_scaled +
      Estimate.Public.administration_scaled,
    Blue_Collar_scaled = Estimate.Natural.resources..construction..and.maintenance.occupations_scaled +
      Estimate.Production..transportation..and.material.moving.occupations_scaled +
      Estimate.Agriculture..forestry..fishing.and.hunting..and.mining_scaled +
      Estimate.Manufacturing_scaled +
      Estimate.Retail.trade_scaled +
      Estimate.Transportation.and.warehousing..and.utilities_scaled,
    Other_Services_scaled = Estimate.Educational.services..and.health.care.and.social.assistance_scaled +
      Estimate.Arts..entertainment..and.recreation..and.accommodation.and.food.services_scaled +
      Estimate.Other.services..except.public.administration_scaled +
      Estimate.Self.employed.in.own.not.incorporated.business.workers_scaled,
    Govt_Benefits_scaled = Estimate.With.Social.Security_scaled +
      Estimate.With.retirement.income_scaled +
      Estimate.With.Supplemental.Security.Income_scaled,
    Welfare_Benefits_scaled = Estimate.With.cash.public.assistance.income_scaled +
      Estimate.With.Food.Stamp.SNAP.benefits.in.the.past.12.months_scaled
  )

# Consolidated Predictor Names
consolidated_predictors <- c(
  "Car_scaled", "Other_Transportation_scaled", "Children_scaled",
  "Corporate_scaled", "Blue_Collar_scaled", "Other_Services_scaled",
  "Govt_Benefits_scaled", "Welfare_Benefits_scaled"
)

# Other Predictors to Keep
other_predictors <- c(
  "Avg_Deal_Size_scaled", "Total_Size_Sq_Ft_scaled", "Fed_Rate_scaled",
  "Unemployment_Rate_scaled", "HomeValue_scaled", "Estimate.Worked.at.home_scaled",
  "Estimate.Mean.travel.time.to.work..minutes._scaled",
  "Estimate.Unpaid.family.workers_scaled", "Estimate.Median.household.income..dollars._scaled",
  "Estimate.With.health.insurance.coverage..With.private.health.insurance_scaled",
  "Estimate.No.health.insurance.coverage_scaled", "Opportunity_Zone_scaled",
  "Residential_Percentage_scaled", "Industrial_Percentage_scaled",
  "Office_Percentage_scaled", "Retail_Percentage_scaled", "Other_Percentage_scaled"
)

# Final Predictors for Modeling
final_predictors <- c(other_predictors, consolidated_predictors)

# ------------------------------
# Step 5: Spatial Projection and Mesh Construction
# ------------------------------

# Prepare spatial data
d_modeling <- d_modeling %>%
  drop_na(lon.census.town, lat.census.town)

d_sf <- st_as_sf(d_modeling, coords = c("lon.census.town", "lat.census.town"), crs = 4326)
d_projected <- st_transform(d_sf, crs = 3310)
projected_coords <- st_coordinates(d_projected)

# Unique Town Coordinates
town_coords_unique <- d_projected %>%
  as.data.frame() %>%
  distinct(Town, .keep_all = TRUE)

coords <- as.matrix(st_coordinates(st_as_sf(town_coords_unique, crs = 3310)))

# Mesh Creation
mesh <- inla.mesh.2d(
  loc = coords,
  max.edge = c(50000, 200000),
  cutoff = 5000
)


# SPDE Model
spde <- inla.spde2.pcmatern(
  mesh = mesh,
  alpha = 2,
  prior.range = c(50000, 0.5),
  prior.sigma = c(1, 0.1)
)

# Create Spatial Index
spatial_index <- inla.spde.make.index("spatial.field", n.spde = spde$n.spde)

# Assign Spatial Indices to Towns
town_to_index <- data.frame(
  Town = town_coords_unique$Town,
  coord_index = 1:nrow(town_coords_unique)
)

d_modeling <- d_modeling %>%
  left_join(town_to_index, by = "Town")

A <- inla.spde.make.A(mesh, loc = coords[d_modeling$coord_index, ])
# ------------------------------
# Step 6: INLA Model
# ------------------------------

# Prepare Data for INLA Stack
predictor_data <- d_modeling %>%
  select(all_of(final_predictors))

covariate_data <- data.frame(
  predictor_data,
  Period = d_modeling$Period,
  Town = d_modeling$Town
)

stack <- inla.stack(
  data = list(y = y),
  A = list(A, 1),
  effects = list(
    spatial.field = spatial_index,
    covariate_data
  )
  tag = "data"
)

# Create INLA Formula
all_predictors_str <- paste(final_predictors, collapse = " + ")
formula_str <- paste0("y ~ -1 + f(spatial.field, model = spde) + f(Period, model = 'ar1') + ", all_predictors_str)
formula <- as.formula(formula_str)

# Fit the INLA Model
result <- inla(
  formula,
  data = inla.stack.data(stack),
  family = "poisson",
  control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE)
)
plot(result)
# ------------------------------
# Step 7: Diagnostics and Visualization
# ------------------------------

summary(result)
fitted_values <- result$summary.fitted.values$mean

# Extract fitted values for observations in the stack
fitted_values <- result$summary.fitted.values$mean[inla.stack.index(stack, tag = "data")$data]

?inla.stack.index


ggplot(data.frame(Observed = y, Fitted = fitted_values), aes(x = Observed, y = Fitted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Observed vs Fitted Deal Volume",
       x = "Observed Deal Volume",
       y = "Fitted Deal Volume") +
  theme_minimal()

```
Some Visualizations
```{r}
fixed_names <- names(result$summary.fixed)

# Plotting posterior marginals for each fixed effect
for (fn in fixed_names) {
  # Marginal distribution of this fixed effect
  marginal <- result$marginals.fixed[[fn]]
  
  # Plot using base R
  plot(marginal, type = "l", main = paste("Posterior Marginal of", fn),
       xlab = fn, ylab = "Density")
  
  # Alternatively, you can compute credible intervals:
  ci_low <- inla.qmarginal(0.025, marginal)
  ci_high <- inla.qmarginal(0.975, marginal)
  abline(v = c(ci_low, ci_high), col = "red", lty = 2)
  
  # You can also print the mean, etc.
  cat(fn, "Mean:", inla.emarginal(function(x) x, marginal), "\n")
}
```

