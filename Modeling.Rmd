---
title: "Modeling"
author: "Matthew Ross"
date: "2024-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(dplyr)
library(tidyr)
library(mgcv)
library(tidygeocoder)
```

```{r}
d <- read.csv("intermediate_for_modeling.csv")

unique_towns <- d %>%
  distinct(Town) %>%
  filter(!is.na(Town)) %>%
  mutate(full_address = paste0(Town, ", California"))

# Geocode towns to get latitude and longitude
town_coords <- unique_towns %>%
  geocode(address = full_address, method = "osm") %>%
  rename(lat.census.town = lat, lon.census.town = long)

# Merge geocoded coordinates back into the main dataframe
d <- d %>%
  left_join(town_coords, by = "Town")

d_new <- d
predictor_data <- read.csv("predictor_data.csv")
```

```{r}
# Load required libraries
library(dplyr)
library(tidyr)
library(lubridate)
library(zoo)
library(geosphere)

# Step 1: Ensure numeric columns in d_new are valid
d_new <- d_new %>%
  mutate(across(starts_with("Estimate."), ~ as.numeric(.), .names = "{.col}"))  # Convert non-numeric to NA

# Step 2: Ensure Year_Month is in Date format and create 6-month periods in d_new
d_new <- d_new %>%
  mutate(
    Year_Month = as.Date(paste0(Year_Month, "-01")),
    Period = floor_date(Year_Month, unit = "6 months")
  )

# Step 3: Ensure Year_Month and Period in predictor_data and validate numeric columns
predictor_data <- predictor_data %>%
  mutate(
    Year_Month = as.Date(paste0(Year_Month, "-01")),
    Period = floor_date(Year_Month, unit = "6 months")
  ) %>%
  mutate(across(
    starts_with("Estimate.") | matches("Opportunity_Zone"),
    ~ as.numeric(.),
    .names = "{.col}"
  ))  # Convert non-numeric to NA

d_aggregated <- d_new %>%
  mutate(
    Asset_Class = case_when(
      Primary.Asset.Type %in% c("Residential", "Industrial", "Office", "Retail") ~ Primary.Asset.Type,
      Primary.Asset.Type %in% c("Niche", "Mixed Use", "Land", "Hotel") ~ "Other",
      TRUE ~ "Unknown"
    )
  ) %>%
  group_by(Town, Period) %>%
  summarise(
    Deal_Volume = n(),
    Avg_Deal_Size = ifelse(Deal_Volume > 0, mean(Deal.Size.Usd.Mn., na.rm = TRUE), NA),
    Total_Size_Sq_Ft = ifelse(Deal_Volume > 0, mean(Total.Size.Sq.Ft., na.rm = TRUE), NA),
    Fed_Rate = mean(Fed_Rate, na.rm = TRUE),
    Unemployment_Rate = mean(Unemployment_Rate, na.rm = TRUE),
    HomeValue = mean(HomeValue, na.rm = TRUE),
    across(starts_with("Estimate."), ~ mean(., na.rm = TRUE), .names = "{.col}"),
    Opportunity_Zone = first(Opportunity_Zone),
    lat.census.town = first(lat.census.town),
    lon.census.town = first(lon.census.town),
    Residential_Percentage = sum(Asset_Class == "Residential") / Deal_Volume * 100,
    Industrial_Percentage = sum(Asset_Class == "Industrial") / Deal_Volume * 100,
    Office_Percentage = sum(Asset_Class == "Office") / Deal_Volume * 100,
    Retail_Percentage = sum(Asset_Class == "Retail") / Deal_Volume * 100,
    Other_Percentage = sum(Asset_Class == "Other") / Deal_Volume * 100,
    .groups = "drop"
  )

# Step 5: Fill in missing periods for each Town
full_grid <- expand.grid(
  Town = unique(d_new$Town),
  Period = seq(min(d_new$Period), max(d_new$Period), by = "6 months")
)

d_aggregated <- full_grid %>%
  left_join(d_aggregated, by = c("Town", "Period"))

# Step 6: Set percentages and metrics to 0 for periods with no deals
d_aggregated <- d_aggregated %>%
  mutate(
    Deal_Volume = ifelse(is.na(Deal_Volume), 0, Deal_Volume),
    Avg_Deal_Size = ifelse(Deal_Volume == 0, 0, Avg_Deal_Size),
    Total_Size_Sq_Ft = ifelse(Deal_Volume == 0, 0, Total_Size_Sq_Ft),
    Residential_Percentage = ifelse(Deal_Volume == 0, 0, Residential_Percentage),
    Industrial_Percentage = ifelse(Deal_Volume == 0, 0, Industrial_Percentage),
    Office_Percentage = ifelse(Deal_Volume == 0, 0, Office_Percentage),
    Retail_Percentage = ifelse(Deal_Volume == 0, 0, Retail_Percentage),
    Other_Percentage = ifelse(Deal_Volume == 0, 0, Other_Percentage)
  )
# Step 7: Perform row-wise updates
d_aggregated <- d_aggregated %>%
  rowwise() %>%
  mutate(
    Fed_Rate = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$Fed_Rate[
        predictor_data$Period == Period
      ], na.rm = TRUE),
      Fed_Rate
    ),
    Unemployment_Rate = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$Unemployment_Rate[
        predictor_data$Period == Period
      ], na.rm = TRUE),
      Unemployment_Rate
    ),
    HomeValue = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$HomeValue[
        predictor_data$Period == Period
      ], na.rm = TRUE),
      HomeValue
    ),
    Opportunity_Zone = ifelse(
      Deal_Volume == 0,
      mean(predictor_data$Opportunity_Zone[
        predictor_data$Town == Town & predictor_data$Period == Period
      ], na.rm = TRUE),
      Opportunity_Zone
    ),
    across(
      starts_with("Estimate."),
      ~ ifelse(
        Deal_Volume == 0,
        mean(predictor_data[[cur_column()]][
          predictor_data$Town == Town & predictor_data$Period == Period
        ], na.rm = TRUE),
        .
      )
    ),
    lat.census.town = first(d_new$lat.census.town[d_new$Town == Town]),
    lon.census.town = first(d_new$lon.census.town[d_new$Town == Town])
  ) %>%
  ungroup()

# Step 8: Forward-backward fill for HomeValue and Estimate.* columns
d_aggregated <- d_aggregated %>%
  group_by(Town) %>%
  mutate(
    HomeValue = ifelse(
      is.na(HomeValue),
      zoo::na.locf(HomeValue, na.rm = FALSE, fromLast = FALSE),
      HomeValue
    ),
    HomeValue = ifelse(
      is.na(HomeValue),
      zoo::na.locf(HomeValue, na.rm = FALSE, fromLast = TRUE),
      HomeValue
    ),
    across(
      starts_with("Estimate."),
      ~ ifelse(
        is.na(.),
        zoo::na.locf(.x, na.rm = FALSE, fromLast = FALSE),
        .
      )
    ),
    across(
      starts_with("Estimate."),
      ~ ifelse(
        is.na(.),
        zoo::na.locf(.x, na.rm = FALSE, fromLast = TRUE),
        .
      )
    )
  ) %>%
  ungroup()

# Step 9: Fill missing values from nearest town if a town has no data for HomeValue or Estimate.*
fill_from_nearest <- function(row, target_column) {
  if (!is.na(row[[target_column]])) {
    return(row[[target_column]])
  }
  
  current_town <- row$Town
  current_period <- row$Period
  current_lat <- row$lat.census.town
  current_lon <- row$lon.census.town
  
  valid_towns <- d_aggregated %>%
    filter(!is.na(!!sym(target_column)) & Period == current_period) %>%
    mutate(distance = distHaversine(cbind(lon.census.town, lat.census.town), 
                                    c(current_lon, current_lat))) %>%
    arrange(distance)
  
  if (nrow(valid_towns) > 0) {
    return(valid_towns[[target_column]][1])
  }
  
  return(NA)
}

# Apply nearest town logic
d_aggregated <- d_aggregated %>%
  rowwise() %>%
  mutate(
    HomeValue = ifelse(is.na(HomeValue), fill_from_nearest(cur_data(), "HomeValue"), HomeValue),
    across(
      starts_with("Estimate."),
      ~ ifelse(is.na(.), fill_from_nearest(cur_data(), cur_column()), .)
    )
  ) %>%
  ungroup()
```
```{r}
library(zoo)
library(geosphere)

# Forward-backward search for Avg_Deal_Size and Total_Size_Sq_Ft within each town
d_aggregated <- d_aggregated %>%
  group_by(Town) %>%
  mutate(
    Avg_Deal_Size = ifelse(
      is.na(Avg_Deal_Size),
      zoo::na.locf(Avg_Deal_Size, na.rm = FALSE, fromLast = FALSE),
      Avg_Deal_Size
    ),
    Avg_Deal_Size = ifelse(
      is.na(Avg_Deal_Size),
      zoo::na.locf(Avg_Deal_Size, na.rm = FALSE, fromLast = TRUE),
      Avg_Deal_Size
    ),
    Total_Size_Sq_Ft = ifelse(
      is.na(Total_Size_Sq_Ft),
      zoo::na.locf(Total_Size_Sq_Ft, na.rm = FALSE, fromLast = FALSE),
      Total_Size_Sq_Ft
    ),
    Total_Size_Sq_Ft = ifelse(
      is.na(Total_Size_Sq_Ft),
      zoo::na.locf(Total_Size_Sq_Ft, na.rm = FALSE, fromLast = TRUE),
      Total_Size_Sq_Ft
    )
  ) %>%
  ungroup()

# Define the function for nearest-town analysis
fill_from_nearest <- function(row, target_column) {
  if (!is.na(row[[target_column]])) {
    return(row[[target_column]])  # Return existing value if it's not NA
  }
  
  # Extract relevant information for the current row
  current_town <- row$Town
  current_period <- row$Period
  current_lat <- row$lat.census.town
  current_lon <- row$lon.census.town
  
  # Find the nearest town with valid data for the target column
  valid_towns <- d_aggregated %>%
    filter(!is.na(!!sym(target_column)) & Deal_Volume > 1 & Period == current_period) %>%
    mutate(distance = distHaversine(cbind(lon.census.town, lat.census.town), 
                                    c(current_lon, current_lat))) %>%
    arrange(distance)  # Sort by distance
  
  if (nrow(valid_towns) > 0) {
    return(valid_towns[[target_column]][1])  # Return the value from the nearest valid town
  }
  
  return(NA)  # Return NA if no valid data is found
}

# Apply nearest-town logic to Avg_Deal_Size and Total_Size_Sq_Ft
d_aggregated <- d_aggregated %>%
  rowwise() %>%
  mutate(
    Avg_Deal_Size = ifelse(is.na(Avg_Deal_Size), fill_from_nearest(cur_data(), "Avg_Deal_Size"), Avg_Deal_Size),
    Total_Size_Sq_Ft = ifelse(is.na(Total_Size_Sq_Ft), fill_from_nearest(cur_data(), "Total_Size_Sq_Ft"), Total_Size_Sq_Ft)
  ) %>%
  ungroup()

# saveRDS(d_aggregated, "data_for_modeling.rds")
```


```{r}
# opt <- options()
# options(pkgType="both")
# install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
# options(opt)
```


```{r}
# ------------------------------
# Step 1: Load Necessary Libraries
# ------------------------------

# Install necessary packages if not already installed
# Uncomment the following lines to install packages
# install.packages(c("dplyr", "glmnet", "INLA", "ggplot2", "tidyr", "car", "sf", "ggmap"))

# Load libraries
library(dplyr)
library(glmnet)
library(INLA)
library(ggplot2)
library(tidyr)
library(car)    # For VIF
library(sf)     # For spatial data handling
library(ggmap)  # For enhanced spatial plots

# ------------------------------
# Step 2: Load and Initialize Data
# ------------------------------

# Load the aggregated data
d_aggregated <- readRDS("data_for_modeling.rds")

# Create a new dataframe for modeling to preserve the original data
d_modeling <- d_aggregated

# ------------------------------
# Step 3: Data Preparation
# ------------------------------

# Define all numeric columns based on dataset structure (columns 5 to 57)
all_numeric_columns <- names(d_modeling)[4:57]

# Exclude latitude and longitude from numeric columns
numeric_columns <- setdiff(all_numeric_columns, c("lat.census.town", "lon.census.town"))

# Define numeric predictors (excluding lat/lon, Period, and Town)
# Assuming 'Period' and 'Town' are not among columns 5:57, else adjust accordingly
numeric_predictors <- numeric_columns

# Convert categorical variables to factors
d_modeling <- d_modeling %>%
  mutate(
    Town = as.factor(Town),
    Period = as.factor(Period)  # Convert to factor if treated as categorical
  )

na_rows <- d_modeling %>%
  select(all_of(numeric_predictors)) %>%
  apply(1, function(x) any(is.na(x)))

d_modeling <- d_modeling[!na_rows, ]
# If Period represents dates, convert to Date type and create a numeric time variable
# Uncomment and adjust the following if necessary
# d_modeling <- d_modeling %>%
#   mutate(
#     Period = as.Date(Period, format = "%Y-%m-%d"),
#     Period_numeric = as.numeric(Period - min(Period)) / 30  # Example: months since start
#   )

# Alternatively, if Period is already ordered or categorical, create a numeric index
d_modeling <- d_modeling %>%
  arrange(Period) %>%
  mutate(Period_numeric = as.numeric(as.factor(Period)))
d_modeling %>% select(Period_numeric)
# Apply normalization (scaling) to numeric predictors
d_modeling <- d_modeling %>%
  mutate(across(all_of(numeric_predictors), scale, .names = "{.col}_scaled"))

# ------------------------------
# Step 4: LASSO Regression for Variable Selection
# ------------------------------

# Define response variable
y <- d_modeling$Deal_Volume

# Create model matrix for LASSO without intercept and excluding lat/lon, Period, and Town
# Ensure that only scaled numeric predictors and Opportunity_Zone are included
X <- model.matrix(
  ~ . -1,  # Include all variables without intercept
  data = d_modeling %>%
    select(ends_with("_scaled"), Opportunity_Zone)
)



dim(X)
dim(d_modeling)

# Calculate correlation matrix
correlation_matrix <- cor(d_modeling %>% select(ends_with("_scaled")))

# Visualize correlation matrix
library(corrplot)
corrplot::corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.6)


# Perform LASSO Regression using cross-validation to find optimal lambda
lasso_model <- cv.glmnet(X, y, alpha = 1, family = "poisson")  # LASSO for variable selection

plot(lasso_model)

# Additionally, plot the coefficient paths
plot(lasso_model$glmnet.fit, xvar = "lambda", label = TRUE)
abline(v = log(lasso_model$lambda.min), col = "red", lty = 2)
abline(v = log(lasso_model$lambda.1se), col = "blue", lty = 2)
legend("topright", legend = c("lambda.min", "lambda.1se"),
       col = c("red", "blue"), lty = 2, cex = 0.8)


# Extract coefficients at the optimal lambda
lasso_coefficients <- coef(lasso_model, s = "lambda.1se")  # Coefficients at optimal lambda

# Extract selected predictors (excluding the intercept)
# Convert sparse matrix to standard matrix
coefficients_matrix <- as.matrix(lasso_coefficients)
# Identify non-zero coefficients
non_zero_indices <- which(coefficients_matrix != 0)

# Extract predictor names (rownames)
selected_predictors <- rownames(coefficients_matrix)[non_zero_indices]

# Remove the intercept if present
selected_predictors <- selected_predictors[selected_predictors != "(Intercept)"]


print(selected_predictors)
cat("Number of Selected Predictors:", length(selected_predictors))

# ------------------------------
# Step 5: Spatio-Temporal Modeling Using INLA
# ------------------------------

# Define spatial coordinates (ensure they are unique)
coords <- d_modeling %>%
  select(lon.census.town, lat.census.town) %>%
  drop_na() %>%             # Remove rows with NA in either longitude or latitude
  distinct() %>%            # Ensure coordinates are unique
  as.matrix() 

# Create spatial mesh
mesh <- inla.mesh.2d(loc = coords, max.edge = c(0.1, 0.5), cutoff = 0.05)

# Define spatial field with a Matern covariance function
spde <- inla.spde2.pcmatern(
  mesh = mesh,
  alpha = 2,  # Smoothness parameter
  prior.range = c(0.1, 0.5),  # Prior for spatial range (mean = 0.1, probability = 0.5)
  prior.sigma = c(1, 0.1)     # Prior for marginal standard deviation (mean = 1, probability = 0.1)
)

# Create spatial index
spatial_index <- inla.spde.make.index("spatial.field", n.spde = spde$n.spde)

# Create projector matrix
A <- inla.spde.make.A(mesh, loc = coords)

# Extract selected predictors for modeling
predictor_data <- d_modeling %>%
  select(all_of(selected_predictors))

d_modeling <- d_modeling %>%
  mutate(
    Town = as.factor(Town),
    Period = as.factor(Period)  # Convert to factor if treated as categorical
  )

# Build INLA stack
stack <- inla.stack(
  data = list(y = y),
  A = list(A, 1),
  effects = list(
    spatial.field = spatial_index,
    predictor_data,
    # Include Period and Town as separate effects if needed
    list(Period = d_modeling$Period),
    list(Town = d_modeling$Town)
  )
)

# Define the spatio-temporal model formula
# Include:
# - Spatial field
# - Temporal autocorrelation via Period_numeric
# - Fixed effects for selected predictors
# - Optional: Random effects for Town if not captured by spatial.field
# Note: Since spatial.field captures spatial dependencies, Town may not need to be included separately
# However, if Town has additional random effects, include it as a separate term

formula <- y ~ 
  f(spatial.field, model = spde) +          # Continuous spatial field
  f(Period, model = "ar1") +                # Temporal autocorrelation
  .                                         # Include all selected predictors as fixed effects

# **Note:** 
# - The `.` in the formula includes all fixed effects from `predictor_data`.
# - `Period` is modeled with an AR1 structure to capture temporal autocorrelation.
# - If `Town` needs to be modeled separately (e.g., as a random effect), it can be added as follows:
#   + f(Town, model = "iid")  # Random effect for Town
# However, if spatial.field sufficiently captures spatial variation, this may not be necessary.

# Fit the INLA model
result <- inla(
  formula,
  data = inla.stack.data(stack),
  family = "poisson",  # Assuming count data for deal volume
  control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE)  # Compute model metrics
)

# Display model summary
summary(result)

# ------------------------------
# Step 6: Visualizations
# ------------------------------

# Temporal Trends Visualization
agg_data <- d_modeling %>%
  group_by(Period, Opportunity_Zone) %>%
  summarise(Total_Deals = sum(Deal_Volume, na.rm = TRUE), .groups = 'drop')

ggplot(agg_data, aes(x = as.Date(Period), y = Total_Deals, color = Opportunity_Zone)) +
  geom_line() +
  labs(title = "Aggregated Deal Volume Over Time by Opportunity Zone",
       x = "Time",
       y = "Total Deals",
       color = "Opportunity Zone") +
  theme_minimal()

# Spatial Trends Visualization with Enhanced Features
# Convert data to sf object for spatial plotting
d_sf <- st_as_sf(d_modeling, coords = c("lon.census.town", "lat.census.town"), crs = 4326)

# Optionally, obtain a basemap using ggmap (requires API key)
# Uncomment and set your API key if you wish to use ggmap
# register_google(key = "YOUR_GOOGLE_MAPS_API_KEY")
# california_map <- get_map(location = "California", zoom = 6)

ggplot() +
  # Uncomment the following line if you have a basemap
  # geom_map(data = california_map, map = california_map, aes(x = lon, y = lat, map_id = id), fill = "white", color = "black", alpha = 0.5) +
  geom_sf(data = d_sf, aes(color = Unemployment_Rate, size = Deal_Volume), alpha = 0.7) +
  scale_color_viridis_c() +
  labs(title = "Spatial Distribution of Deal Volume and Unemployment Rate",
       x = "Longitude",
       y = "Latitude",
       size = "Total Deals",
       color = "Unemployment Rate") +
  theme_minimal()

# ------------------------------
# Step 7: Model Diagnostics
# ------------------------------

# Check for Multicollinearity among Selected Predictors using VIF
# Note: VIF is typically used for linear models; interpret with caution for count models
lm_model <- lm(Deal_Volume ~ ., data = d_modeling %>% select(all_of(selected_predictors, "Deal_Volume")))
vif_values <- vif(lm_model)
print("Variance Inflation Factor (VIF) for Selected Predictors:")
print(vif_values)

# Optional: Remove predictors with high VIF (>5 or 10) and refit LASSO/INLA if necessary

# Posterior Predictive Checks: Compare Observed vs Fitted Values
fitted_values <- result$summary.fitted.values$mean
ggplot(data.frame(Observed = y, Fitted = fitted_values), aes(x = Observed, y = Fitted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Observed vs Fitted Deal Volume",
       x = "Observed Deal Volume",
       y = "Fitted Deal Volume") +
  theme_minimal()

# Residuals Analysis
residuals <- y - fitted_values
ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Residuals Distribution",
       x = "Residuals",
       y = "Frequency") +
  theme_minimal()

# Dispersion Check for Overdispersion
dispersion_ratio <- sum(residuals^2) / length(residuals)
print(paste("Dispersion Ratio:", round(dispersion_ratio, 2)))

# If dispersion_ratio > 1.5, consider using Negative Binomial
if (dispersion_ratio > 1.5) {
  result_nb <- inla(
    formula,
    data = inla.stack.data(stack),
    family = "nbinomial",  # Negative Binomial family
    control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
    control.compute = list(dic = TRUE, waic = TRUE)
  )
  
  # Display summary of Negative Binomial model
  summary(result_nb)
  
  # Compare Model Metrics
  print("Negative Binomial Model Summary:")
  print(summary(result_nb))
}

# ------------------------------
# Step 8: Additional Enhancements (Optional)
# ------------------------------

# Example: Visualizing Fixed Effects with Confidence Intervals
fixed_effects <- result$summary.fixed
fixed_effects_df <- as.data.frame(fixed_effects)
fixed_effects_df$Predictor <- rownames(fixed_effects_df)

ggplot(fixed_effects_df, aes(x = reorder(Predictor, mean), y = mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = `0.025quant`, ymax = `0.975quant`), width = 0.2) +
  coord_flip() +
  labs(title = "Fixed Effects Estimates",
       x = "Predictors",
       y = "Effect Size") +
  theme_minimal()

# ------------------------------
# Step 9: Review Column Names (Optional)
# ------------------------------

# Display column names of the modeling dataframe
print("Column Names of d_modeling:")
print(colnames(d_modeling))

```


